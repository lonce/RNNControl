{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AudioDataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3b2e585bc44e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudioDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mAudioDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulawnEncode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray2tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic2tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minjectNoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalizeDim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparamManager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparamManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'AudioDataloader'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "import audioDataloader.dataloader as dataloader\n",
    "from audioDataloader.transforms import mulawnEncode,mulaw,array2tensor,dic2tensor,injectNoise,normalizeDim\n",
    "from paramManager import paramManager\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Params  \n",
    "<a id=\"dataparams\"></a>\n",
    "\n",
    "These parameters are pickled to file, and serve several purposes:\n",
    "* The allow other programs to properly evaluate and visualize the trained (and also saved) models,\n",
    "* PRovide a record of the parameters that allow reproducing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmodel = False #True means load params from disk, and use last saved model running from last saved step \n",
    "restartPickledParamFilePath = 'model' + '/2018-11-22_11-41-42_modelParams.pkl'\n",
    "\n",
    "if loadmodel == True :  # ------------   get params from file\n",
    "    with open(restartPickledParamFilePath, 'rb') as input:\n",
    "        params = pickle.load(input) \n",
    "    loadmodelfile=params['savemodeldir'] + '/' + params['savedModel']\n",
    "    #override any of the saved params\n",
    "    params['max_steps']= 2000 #in addition to whatever was already run\n",
    "    \n",
    "else : # ---------------------    New run, set the params\n",
    "    sourcedatadir='/tmp/synth.64.76.ConstSlopeTransients_Even.1.len.6.1.0_Odd.01.len.6.2.0'\n",
    "    #sourcedatadir='/tmp/nSynthTransients'\n",
    "    #sourcedatadir=\"/home/lonce/ZCODE/PYTORCH/data/nsynth.64.76.dl\"    \n",
    "    #sourcedatadir=\"/tmp/nsynth_sm_scaled\"\n",
    " \n",
    "\n",
    "    params = dict(\n",
    "        # Read/write directory of data & parameter files\n",
    "        #*************************************\n",
    "        sample_rate=16000,\n",
    "        runTimeStamp='{:%Y-%m-%d_%H-%M-%S}'.format(datetime.now()),\n",
    "        \n",
    "        datadir = sourcedatadir,\n",
    "        #without a csv file, all data in datadir are used for training\n",
    "        #csvfile=sourcedatadir+\"/64_76.csv\",\n",
    "        \n",
    "        paramdir = sourcedatadir + '/dataparams',\n",
    "\n",
    "\n",
    "        # Save & load parameters\n",
    "        #*************************************\n",
    "        #--- Note all intervals below are counted in no. of steps. 1 epoch = [len(dataset)//batch_size] steps ---\n",
    "\n",
    "        savemodel = True,\n",
    "        savemodel_interval = 10, #200,  #If 0 (and savemodel=True) will only save model at the end of entire training\n",
    "        savemodeldir = os.getcwd() + '/model', #default saving directory for models and the parameterization\n",
    "        savedModel='', #updated at savemodel_interval\n",
    "        savedSteps=0,  #updated at savemodel_interval\n",
    "        \n",
    "        max_steps = 100, #1000,  #10000, #max number batches of steps per epoch (typically num_epochs=1)\n",
    "\n",
    "\n",
    "        # Training parameters\n",
    "        #*************************************\n",
    "        noise=.1,\n",
    "        seqLen = 256,\n",
    "        stride = 1,\n",
    "        batch_size = 256,\n",
    "        num_epochs = 1,\n",
    "        lr = 0.005,\n",
    "        props=['instID','pressure1','midiPitch'],\n",
    "        \n",
    "        # Model parameters\n",
    "        hiddenSize = 40, #100,\n",
    "        nLayers = 4,\n",
    "        \n",
    "        lowNote=64, #52,\n",
    "        hiNote=76, #86\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = params['sample_rate']\n",
    "\n",
    "max_steps = params['max_steps'] #set max_steps > (len(dataset)//batch_size) if training for more than 1 epoch\n",
    "k_previousRunSteps=params['savedSteps']\n",
    "log_interval = 50\n",
    "visualize_interval = 100\n",
    "\n",
    "#Generation parameters\n",
    "#*************************************\n",
    "max_length = params['seqLen']*3\n",
    "\n",
    "# Cuda\n",
    "#*************************************\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "#*************************************\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def mydate() :\n",
    "    return (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the available conditional parameters first\n",
    "#*************************************\n",
    "pm = paramManager.paramManager(params['datadir'], params['paramdir'])\n",
    "datafiles = pm.filenames(params['datadir'])\n",
    "paramfile = pm.getParams(datafiles[0]) \n",
    "\n",
    "notpresent=[x for x in params['props'] if x not in paramfile.keys()]\n",
    "assert 0 == len(notpresent), \"props {} are not in the dataparam files\".format(notpresent)\n",
    "\n",
    "print(paramfile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset & dataloader\n",
    "#*************************************\n",
    "audiocoding = mulawnEncode(256,0,1) #initialize the mu-law encodings\n",
    "targetcoding = mulaw(256)\n",
    "rescalePitch = normalizeDim('midiPitch',params['lowNote'],params['hiNote'])\n",
    "rescalePressure1 = normalizeDim('pressure1',0,0.9)\n",
    "#rescalePressure2 = normalizeDim('pressure2',.1,1)\n",
    "\n",
    "adataset = dataloader.AudioDataset(sr,params['seqLen'],params['stride'],\n",
    "                                  csvfile = None if 'csvfile' not in params else params['csvfile'],    \n",
    "                                  datadir  = None if 'csvfile' in params else params['datadir'],\n",
    "                                  extension= None if 'csvfile' in params else 'wav',\n",
    "                                  paramdir=params['paramdir'],prop=params['props'],\n",
    "                                  transform=transform.Compose([injectNoise(weight=params['noise']),audiocoding,array2tensor(torch.FloatTensor)]),\n",
    "                                  param_transform=transform.Compose([ rescalePitch,dic2tensor(torch.FloatTensor)]),\n",
    "                                  target_transform=transform.Compose([targetcoding,array2tensor(torch.LongTensor)]))\n",
    "\n",
    "testdataset = dataloader.AudioDataset(sr,params['seqLen'],params['stride'],\n",
    "                                  csvfile = None if 'csvfile' not in params else params['csvfile'],    \n",
    "                                  datadir  = None if 'csvfile' in params else params['datadir'],\n",
    "                                  extension= None if 'csvfile' in params else 'wav',\n",
    "                                  paramdir=params['paramdir'],prop=params['props'],\n",
    "                                  transform=transform.Compose([array2tensor(torch.FloatTensor)]), \n",
    "                                  param_transform=transform.Compose([ rescalePitch,dic2tensor(torch.FloatTensor)]),\n",
    "                                  target_transform=transform.Compose([array2tensor(torch.LongTensor)]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=adataset,\n",
    "                                           batch_size=params['batch_size'], \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#just for testing\n",
    "print(len(adataset))\n",
    "\n",
    "for i in range(len(adataset)):\n",
    "    sample,target = adataset[i]\n",
    "    #plt.plot(sample['audio'])\n",
    "    print(sample)\n",
    "    #print(target)\n",
    "    if i == 1:\n",
    "        break #just to visualize, note: audio is already mu-lawed\n",
    "\"\"\"\n",
    "\n",
    "inp,target = testdataset[np.random.randint(len(testdataset))]\n",
    "print(inp)\n",
    "print(target.shape)\n",
    "#print(paramdict)\n",
    "\n",
    "inpn = inp[:,0].numpy()\n",
    "\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "#print(\"input shape\",inp.shape)\n",
    "#input dim: (batch, seq, feature)\n",
    "\n",
    "\n",
    "print(\"random input:\")\n",
    "print(\"original\")\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(inpn)), inpn) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mu_samp = audiocoding(inp[:,0])\n",
    "print(mu_samp.shape)\n",
    "inp[:,0] = mu_samp\n",
    "print(inp)\n",
    "inpn_mu = inp[:,0].numpy()\n",
    "\n",
    "print(\"mu-law\")\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(inpn_mu)), inpn_mu) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "inp,target,param = adataset.apply_transforms(inp,target,paramdict)\n",
    "inpn = inp.numpy()\n",
    "print(\"mu-law\")\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(inpn)), inpn) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "input = torch.cat((inp,param),1)\n",
    "print(input)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of steps per epoch is\",len(adataset)//params['batch_size'])\n",
    "\n",
    "samp = adataset.rand_sample()\n",
    "print(samp.shape)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "samp = audiocoding(samp)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "#*************************************\n",
    "class RNN(nn.Module):\n",
    "    # input size - the number of \"classes\"\n",
    "    def __init__(self, input_size, cond_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.cond_size = cond_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers #no. of stacked GRU layers\n",
    "\n",
    "        self.i2h = nn.Linear(input_size+cond_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "\n",
    "    # input and cv are each one sequence element \n",
    "    def forward(self, input, hidden, batch_size=1):\n",
    "        #print(\"input size is \" + str((input.size())))\n",
    "        \n",
    "        h1 = self.i2h(input)\n",
    "        #print(\"size of h1 is \" + str(h1.size()))\n",
    "        \n",
    "        h_out, hidden = self.gru(h1.view(batch_size,1,-1), hidden)\n",
    "        #print(\"h_out\"+str(h_out.size()))\n",
    "        \n",
    "        output = self.decoder(h_out.view(batch_size,-1))\n",
    "        #print(\"output2\"+str(output.size()))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # initialize hiddens for each minibatch\n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return .1*torch.rand(self.n_layers, batch_size, self.hidden_size, dtype=torch.float, device=device)-.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "def train(model,epoch):\n",
    "    model.train() #put in training mode\n",
    "    ave_loss_over_steps = 0\n",
    "    \n",
    "    for step, (inp,target) in enumerate(train_loader):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        hidden = model.init_hidden(params['batch_size'])\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(params['seqLen']):\n",
    "            outputs, hidden = model(inp[:,i,:],hidden,params['batch_size'])  #input dim: (batch, seq, feature)\n",
    "            loss += criterion(outputs, torch.squeeze(target[:,i],1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ave_loss_per_sample = loss.item()/params['seqLen']   #over each minibatch\n",
    "        ave_loss_over_steps += ave_loss_per_sample\n",
    "        \n",
    "        if (step+1) % log_interval == 0:\n",
    "            print ('{:%Y-%m-%d %H:%M:%S} Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}'.format( \n",
    "                datetime.now(), epoch+1, params['num_epochs'], step+1, len(adataset)//params['batch_size'], ave_loss_over_steps/log_interval))\n",
    "            \n",
    "            list_of_losses.append(ave_loss_over_steps/log_interval)\n",
    "            ave_loss_over_steps = 0\n",
    "            \n",
    "        if (step+1) % visualize_interval == 0:\n",
    "            result = generate(model,max_length)\n",
    "            plt.figure(figsize=(20,1)) \n",
    "            plt.plot(np.arange(len(result)), result) #just print one example from the batch\n",
    "            plt.show()\n",
    "            model.train() #put model back to training mode\n",
    "            \n",
    "        if (step+1) % params['savemodel_interval'] == 0 and params['savemodel'] and params['savemodel_interval'] != 0:\n",
    "            lastSavedStep=k_previousRunSteps+step+1\n",
    "            lastSavedModel=params['runTimeStamp']+'_model_epoch{}_step{}.pth'.format(epoch+1,lastSavedStep)\n",
    "            torch.save(model.state_dict(), params['savemodeldir'] + '/' + lastSavedModel)\n",
    "            print('model saved at epoch {} step {} for this run ({} overall)'.format(epoch+1,step+1, lastSavedStep))\n",
    "\n",
    "            #write (overwrite) the param file for each save so we always have the latest, even if run doesn't complete\n",
    "            params['savedSteps']=lastSavedStep\n",
    "            params['savedModel']=lastSavedModel\n",
    "            with open(params['savemodeldir'] + '/' + params['runTimeStamp'] + '_modelParams.pkl', 'wb') as output:\n",
    "                    pickle.dump(params, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                \n",
    "        if step>=max_steps:\n",
    "            break\n",
    "            \n",
    "        \n",
    "        \n",
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "def generate(model,max_length,primer=None,paramvect=None, returnHiddenSequence=False):\n",
    "    \"\"\"Generate a signal using the provided model.\n",
    "    @param max_length of the synthesized signal\n",
    "    @param primer a torch.tensor of shape (batch=1, primer_signal=primer_length, num_inputs=1+num_cond_parameters)\n",
    "    @paramvect conditioning parameters  (a function of i, sample number)\n",
    "    \"\"\" \n",
    "    if returnHiddenSequence :\n",
    "        hs=[]\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p_inp,target in test_loader:\n",
    "            if primer is not None:\n",
    "                # must clone else primer is changed outside this function\n",
    "                p_inp.data=primer.clone()\n",
    "            seq = np.copy(p_inp[0,:,0])  #extract the original sample\n",
    "            \n",
    "            seq_mu = audiocoding(seq)  #mu-law\n",
    "            p_inp[0,:,0] = array2tensor(torch.FloatTensor)(seq_mu) #now we have both the original and mu-lawed samples\n",
    "            break   \n",
    "        generated = seq # before mu-law audio encoding\n",
    "       \n",
    "        p_inp = p_inp.to(device)\n",
    "        #print(\"p_inp\",p_inp)\n",
    "\n",
    "        hidden = model.init_hidden()\n",
    "        if returnHiddenSequence :\n",
    "            hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "\n",
    "            \n",
    "        for j in range(params['seqLen']-1):  #build up hidden state\n",
    "            _, hidden = model(p_inp[:,j,:],hidden)\n",
    "        inp = p_inp[:,-1,:]  #feed the last value as the initial value of the actual generation\n",
    "        \n",
    "        #print(\"last inp from primer\",inp)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            outputs, hidden = model(inp,hidden)\n",
    "            \n",
    "            outputs = nn.functional.log_softmax(outputs,dim=1)\n",
    "            topv, topi = outputs.detach().topk(1)  #choose the strongest activation\n",
    "            #print(topv,topv.shape)\n",
    "            predicted_sample = targetcoding.index2float(topi)\n",
    "            \n",
    "            generated = np.append(generated,predicted_sample)\n",
    "            \n",
    "            inp[:,0] = torch.from_numpy(audiocoding([predicted_sample])).type(torch.cuda.FloatTensor)\n",
    "            #print(\"input\",inp)\n",
    "            #print(\"shape\",inp.shape)\n",
    "            if paramvect is not None:\n",
    "                inp[:,1:] = torch.tensor(paramvect(i))\n",
    "                #print(\"input2\",inp)\n",
    "                \n",
    "            if returnHiddenSequence :\n",
    "                hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "                                       \n",
    "        if returnHiddenSequence :\n",
    "            return generated, hs\n",
    "        else :\n",
    "            return generated\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#dataparams\">Go To Data Params</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, optimizer and objective func\n",
    "#*************************************\n",
    "rnn = RNN(input_size=1,cond_size=len(params['props']),hidden_size=params['hiddenSize'],output_size=256,n_layers=params['nLayers']).to(device)\n",
    "if loadmodel: # load checkpoint if needed\n",
    "    print(\"Loading existing checkpoint...\")\n",
    "    rnn.load_state_dict(torch.load(loadmodelfile))\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "n_batches = 12\n",
    "print(\"Do it! {}\".format(mydate()))\n",
    "\n",
    "for bat, sample_batched in enumerate(train_loader):\n",
    "    print(\"bat num {} at time {}\".format(bat, mydate()))\n",
    "    if bat >= n_batches :\n",
    "        break;\n",
    "print(\"finished at {}\".format(mydate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "#*************************************\n",
    "list_of_losses = []\n",
    "\n",
    "print('{:%Y-%m-%d %H:%M:%S} Starting training...'.format(datetime.now()))\n",
    "start_time = time.monotonic()\n",
    "for epoch in range(params['num_epochs']):\n",
    "    train(rnn,epoch)\n",
    "elapsed_time = time.monotonic() - start_time\n",
    "print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "if params['savemodel_interval'] == 0 and params['savemodel']:\n",
    "    torch.save(model.state_dict(), \n",
    "       '{}/{}_model_epoch{}.pth'.format(params['savemodeldir'],params['runTimeStamp'],params['num_epochs']))\n",
    "    print('model saved at epoch{}'.format(params['num_epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "#*************************************\n",
    "plt.figure()\n",
    "plt.plot(list_of_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "snn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
